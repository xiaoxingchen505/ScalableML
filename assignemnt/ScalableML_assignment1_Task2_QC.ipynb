{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Assignment 1 Task2 QC\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setCheckpointDir('checkpoint/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.read.text(\"ml-25m/ratings.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "\n",
    "header = parts.first()\n",
    "parts = parts.filter(lambda line: line != header)\n",
    "\n",
    "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),rating=float(p[2]), timestamp=int(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "\n",
    "lines = spark.read.text(\"ml-25m/genome-scores.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "header = parts.first()\n",
    "parts = parts.filter(lambda line: line != header)\n",
    "\n",
    "scoresRDD = parts.map(lambda p: Row(movieId=int(p[0]), tagId=int(p[1]),relevance=float(p[2])))\n",
    "scores = spark.createDataFrame(scoresRDD)\n",
    "\n",
    "lines = spark.read.text(\"ml-25m/genome-tags.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "header = parts.first()\n",
    "parts = parts.filter(lambda line: line != header)\n",
    "\n",
    "tagsRDD = parts.map(lambda p: Row(tagId=int(p[0]), tag=str(p[1])))\n",
    "tags = spark.createDataFrame(tagsRDD)\n",
    "\n",
    "lines = spark.read.text(\"ml-25m/tags.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "header = parts.first()\n",
    "parts = parts.filter(lambda line: line != header)\n",
    "\n",
    "count_tagsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),tag=str(p[2]), timestamp=int(p[3])))\n",
    "count_tags = spark.createDataFrame(count_tagsRDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fold_0, fold_1, fold_2) = ratings.randomSplit([1.0, 1.0, 1.0],seed=1)\n",
    "\n",
    "data_list = [fold_0,fold_1,fold_2]\n",
    "test_list = [fold_2,fold_0,fold_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=10, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(als_version):\n",
    "    dfItemFactors_list = []\n",
    "    fold_j = 0\n",
    "    for fold_i in range(len(data_list)):\n",
    "        if fold_j < len(data_list)-1:\n",
    "            fold_j+= 1\n",
    "        else:\n",
    "            fold_j = 0\n",
    "        first_train = data_list[fold_i].union(data_list[fold_j])\n",
    "        model = als_version.fit(first_train)\n",
    "        dfItemFactors=model.itemFactors\n",
    "        dfItemFactors_list.append(dfItemFactors)\n",
    "    return dfItemFactors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItemFactors_list = cross_validate(als)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[0],Vectors.dense(r[1])]).toDF(['movieId','features'])\n",
    "                                                                   \n",
    "df_1_vec= transData(ItemFactors_list[0])\n",
    "df_2_vec= transData(ItemFactors_list[1])\n",
    "df_3_vec= transData(ItemFactors_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.clustering import KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "kmeans = KMeans().setK(25).setSeed(1)\n",
    "model_1= kmeans.fit(df_1_vec)\n",
    "model_2= kmeans.fit(df_2_vec)\n",
    "model_3= kmeans.fit(df_3_vec)\n",
    "predictions_1 = model_1.transform(df_1_vec)\n",
    "predictions_2 = model_2.transform(df_2_vec)\n",
    "predictions_3 = model_3.transform(df_3_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieid_cluster_1 = predictions_1.drop('features')\n",
    "movieid_cluster_2 = predictions_2.drop('features')\n",
    "movieid_cluster_3 = predictions_3.drop('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieid_cluster_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_cluster_1 = predictions_1.groupBy('prediction').count().sort('count', ascending=False).limit(3)\n",
    "largest_cluster_2 = predictions_2.groupBy('prediction').count().sort('count', ascending=False).limit(3)\n",
    "largest_cluster_3 = predictions_3.groupBy('prediction').count().sort('count', ascending=False).limit(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Split:\")\n",
    "largest_cluster_1.show()\n",
    "print(\"Second Split:\")\n",
    "largest_cluster_2.show()\n",
    "print(\"Third Split:\")\n",
    "largest_cluster_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dataset \n",
    "first_cluster_1 = movieid_cluster_1.filter(movieid_cluster_1['prediction'].contains(\"11\"))\n",
    "second_cluster_1 = movieid_cluster_1.filter(movieid_cluster_1['prediction'].contains(\"13\"))\n",
    "third_cluster_1 = movieid_cluster_1.filter(movieid_cluster_1['prediction'].contains(\"14\"))\n",
    "\n",
    "first_cluster_2 = movieid_cluster_2.filter(movieid_cluster_2['prediction'].contains(\"15\"))\n",
    "second_cluster_2 = movieid_cluster_2.filter(movieid_cluster_2['prediction'].contains(\"22\"))\n",
    "third_cluster_2 = movieid_cluster_2.filter(movieid_cluster_2['prediction'].contains(\"11\"))\n",
    "\n",
    "first_cluster_3 = movieid_cluster_3.filter(movieid_cluster_3['prediction'].contains(\"16\"))\n",
    "second_cluster_3 = movieid_cluster_3.filter(movieid_cluster_3['prediction'].contains(\"3\"))\n",
    "third_cluster_3 = movieid_cluster_3.filter(movieid_cluster_3['prediction'].contains(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use inner join to have all the tags for the movies in the cluster \n",
    "first_cluster_scores_1 = first_cluster_1.join(scores, on=['movieId'], how='inner')\n",
    "second_cluster_scores_1 = second_cluster_1.join(scores, on=['movieId'], how='inner')\n",
    "third_cluster_scores_1 = third_cluster_1.join(scores, on=['movieId'], how='inner')\n",
    "\n",
    "first_cluster_scores_2 = first_cluster_2.join(scores, on=['movieId'], how='inner')\n",
    "second_cluster_scores_2 = second_cluster_2.join(scores, on=['movieId'], how='inner')\n",
    "third_cluster_scores_2 = third_cluster_2.join(scores, on=['movieId'], how='inner')\n",
    "\n",
    "first_cluster_scores_3 = first_cluster_3.join(scores, on=['movieId'], how='inner')\n",
    "second_cluster_scores_3 = second_cluster_3.join(scores, on=['movieId'], how='inner')\n",
    "third_cluster_scores_3 = third_cluster_3.join(scores, on=['movieId'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the top 3 tags by adding all the scores, fc,sc,tc refer to first,second,third clusters.\n",
    "largest_tags_fc_1 = first_cluster_scores_1.groupBy('tagId').agg(func.sum('relevance'))\n",
    "largest_tags_fc_2 = second_cluster_scores_1.groupBy('tagId').agg(func.sum('relevance'))\n",
    "largest_tags_fc_3 = third_cluster_scores_1.groupBy('tagId').agg(func.sum('relevance'))\n",
    "\n",
    "largest_tags_sc_1 = first_cluster_scores_2.groupBy('tagId').agg(func.sum('relevance'))\n",
    "largest_tags_sc_2 = second_cluster_scores_2.groupBy('tagId').agg(func.sum('relevance'))\n",
    "largest_tags_sc_3 = third_cluster_scores_2.groupBy('tagId').agg(func.sum('relevance'))\n",
    "\n",
    "largest_tags_tc_1  = first_cluster_scores_3.groupBy('tagId').agg(func.sum('relevance'))\n",
    "largest_tags_tc_2 = second_cluster_scores_3.groupBy('tagId').agg(func.sum('relevance'))\n",
    "largest_tags_tc_3 = third_cluster_scores_3.groupBy('tagId').agg(func.sum('relevance'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------   First Split     --------------------------\")\n",
    "largest_tags_fc_1.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "largest_tags_fc_2.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "largest_tags_fc_3.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "print(\"---------------------   Second Split    --------------------------\")\n",
    "largest_tags_sc_1.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "largest_tags_sc_2.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "largest_tags_sc_3.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "print(\"---------------------   Third Split     --------------------------\")\n",
    "largest_tags_tc_1.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "largest_tags_tc_2.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "largest_tags_tc_3.sort('sum(relevance)', ascending=False).show(3,False)\n",
    "print(\"------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster_scores_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster_scores_1.filter(first_cluster_scores_1['tagId'].contains(\"742\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cluster_scores_1.filter(first_cluster_scores_1['tagId'].contains(\"646\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------------------TOP 3 Tag for each cluster and split------------------------\")\n",
    "print(\"---------------------First Split First Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"468\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"468\")).count())\n",
    "\n",
    "print(\"---------------------First Split Second Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"188\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"188\")).count())\n",
    "\n",
    "\n",
    "print(\"---------------------First Split Third Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"807\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"807\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"792\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"792\")).count())\n",
    "\n",
    "print(\"---------------------Second Split First Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"323\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"323\")).count())\n",
    "\n",
    "print(\"---------------------Second Split Second Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"807\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"807\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "print(\"---------------------Second Split Third Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"1104\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"1104\")).count())\n",
    "\n",
    "print(\"---------------------Third Split First Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"445\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"445\")).count())\n",
    "\n",
    "print(\"---------------------Third Split Second Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"702\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"702\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"1104\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"1104\")).count())\n",
    "\n",
    "print(\"---------------------Third Split Third Cluster------------------------\")\n",
    "tags.filter(tags['tagId'].contains(\"742\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"742\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"646\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"646\")).count())\n",
    "\n",
    "tags.filter(tags['tagId'].contains(\"188\")).show()\n",
    "# print('Respective Number of movies having the tags:',count_tags.filter(count_tags['tagId'].contains(\"188\")).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
