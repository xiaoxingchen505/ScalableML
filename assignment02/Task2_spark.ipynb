{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row,Column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import when,log,exp\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Assignment Task2\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.executor.cores\", 8)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#start Data pre-processing\n",
    "raw_df = spark.read.csv('./Dataset/ClaimPredictionChallenge/train_set.csv',header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df =raw_df.select('Vehicle','Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1','Cat2','Cat3','Cat4','Cat5','Cat6','Cat7','Cat8','Cat9','Cat10','Cat11','Cat12','Calendar_Year','Model_Year','Claim_Amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in raw_df.columns:\n",
    "    raw_df = raw_df.filter((raw_df[col] != '?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "double_data = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4']\n",
    "int_data = ['Vehicle','Calendar_Year','Model_Year','Claim_Amount']\n",
    "input_features = double_data+int_data\n",
    "for col in double_data:\n",
    "    raw_df = raw_df.withColumn(col, raw_df[col].cast(DoubleType()))\n",
    "for col in int_data:\n",
    "    raw_df = raw_df.withColumn(col, raw_df[col].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.withColumn(int_data[0], (raw_df[int_data[0]]-2005))\n",
    "raw_df = raw_df.withColumn(int_data[1], (raw_df[int_data[1]]-1981))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = {'Cat1':11,'Cat2':4,'Cat3':7,'Cat4':4,'Cat5':4,'Cat6':7,'Cat7':5,'Cat8':4,'Cat9':2,'Cat10':4,'Cat11':7,'Cat12':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainingData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bf4ea5449597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrainingData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtestData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainingData' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer,OneHotEncoderEstimator\n",
    "\n",
    "for col,num in categorical_features.items():\n",
    "    name = col+'_id'\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=name)\n",
    "    trainingData = indexer.fit(trainingData).transform(trainingData)\n",
    "    testData = indexer.fit(testData).transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.select('Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8',\\\n",
    "                        'NVVar1','NVVar2','NVVar3','NVVar4',\\\n",
    "                        'Cat1_id','Cat2_id','Cat3_id','Cat4_id','Cat5_id','Cat6_id','Cat7_id','Cat8_id','Cat9_id','Cat10_id','Cat11_id','Cat12_id',\\\n",
    "                        'Calendar_Year','Model_Year','Claim_Amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = ['Cat1_id','Cat2_id','Cat3_id','Cat4_id','Cat5_id','Cat6_id','Cat7_id','Cat8_id','Cat9_id','Cat10_id','Cat11_id','Cat12_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ohe = []\n",
    "for col in category_id:\n",
    "    cat_ = col.replace('_id','_ohe')\n",
    "    cat_ohe.append(cat_)\n",
    "    input_features.append(cat_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_df.select('Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1_id','Cat2_id','Cat3_id','Cat4_id','Cat5_id','Cat6_id','Cat7_id','Cat8_id','Cat9_id','Cat10_id','Cat11_id','Cat12_id','Calendar_Year','Model_Year','Claim_Amount')\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=category_id, outputCols=cat_ohe)\n",
    "\n",
    "encoder_data = encoder.fit(data)\n",
    "data_ohe = encoder_data.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble cat features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "Ohe_Assembler = VectorAssembler(inputCols = cat_ohe, outputCol = 'cat_features_assembled')\n",
    "cat_data_ohe = Ohe_Assembler.transform(data_ohe)\n",
    "\n",
    "#assemble all features\n",
    "# all_features_assembler = VectorAssembler(inputCols=['cat_features_assembled','Var1','Var2','Var3','Var4','Var5',\\\n",
    "#                                           'Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4'],\\\n",
    "#                                 outputCol='features')\n",
    "# all_data = all_features_assembler.transform(cat_data_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0cd65f1f8801>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'all_data' is not defined"
     ]
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "t_data= all_data.select('features','Claim_Amount')\n",
    "\n",
    "(trainingData, testData) = t_data.randomSplit([0.8, 0.2],47)\n",
    "\n",
    "\n",
    "trainingData.cache()  \n",
    "testData.cache()\n",
    "print('Data preprocessing finished.')\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Claim_Amount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 79.08290982246399\n",
      "RMSE = 39.7016\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "print('Start training......')\n",
    "lr_Model = lr.fit(trainingData)\n",
    "lr_prediction = lr_Model.transform(testData)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"Claim_Amount\",\\\n",
    "                                predictionCol=\"prediction\",\\\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "lr_rmse = evaluator.evaluate(lr_prediction)\n",
    "end = time.time()\n",
    "print('Execution time:',end-start)\n",
    "print(\"RMSE = %g\" % lr_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
