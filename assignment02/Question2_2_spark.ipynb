{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row,Column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pyspark.sql.functions import when,log,exp\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoderEstimator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Assignment Task2\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.executor.cores\", 8)\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start data preprocessing...\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'cannot resolve \\'`Vehicle`\\' given input columns: [7z��\\'\\x1c\\x00\\x03���K�Ֆ\\x06\\x00\\x00\\x00\\x00a\\x00\\x00\\x00\\x00\\x00\\x00\\x00@�e�\\x00)\\x1b��\\x00ora�A\\\\�\\x00e\\x04\\x04�G�Hg���\\x02�\\x05�G�\\'ۍі�S��c=w7��\\x11~�n�O@�a�o2f�t��U�VVP��\\x17ty��, ��W���2�\\'~X�l�6�y\\x18\\x1e8q����e\\x01-��%.μ\\x1ex�\\x06��\" �\\x1cvhE��8?������K�];;\\n\\'Project [\\'Vehicle, \\'Var1, \\'Var2, \\'Var3, \\'Var4, \\'Var5, \\'Var6, \\'Var7, \\'Var8, \\'NVVar1, \\'NVVar2, \\'NVVar3, \\'NVVar4, \\'Cat1, \\'Cat2, \\'Cat3, \\'Cat4, \\'Cat5, \\'Cat6, \\'Cat7, \\'Cat8, \\'Cat9, \\'Cat10, \\'Cat11, ... 4 more fields]\\n+- Relation[7z��\\'\\x1c\\x00\\x03���K�Ֆ\\x06\\x00\\x00\\x00\\x00a\\x00\\x00\\x00\\x00\\x00\\x00\\x00@�e�\\x00)\\x1b��\\x00ora�A\\\\�\\x00e\\x04\\x04�G�Hg���\\x02�\\x05�G�\\'ۍі�S��c=w7��\\x11~�n�O@�a�o2f�t��U�VVP��\\x17ty��#3367,��W���2�\\'~X�l�6�y\\x18\\x1e8q����e\\x01-��%.μ\\x1ex�\\x06��\" �\\x1cvhE��8?������K�#3368] csv\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1915.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`Vehicle`' given input columns: [7z��'\u001c\u0000\u0003���K�Ֆ\u0006\u0000\u0000\u0000\u0000a\u0000\u0000\u0000\u0000\u0000\u0000\u0000@�e�\u0000)\u001b��\u0000ora�A\\�\u0000e\u0004\u0004�G�Hg���\u0002�\u0005�G�'ۍі�S��c=w7��\u0011~�n�O@�a�o2f�t��U�VVP��\u0017ty��, ��W���2�'~X�l�6�y\u0018\u001e8q����e\u0001-��%.μ\u001ex�\u0006��\" �\u001cvhE��8?������K�];;\n'Project ['Vehicle, 'Var1, 'Var2, 'Var3, 'Var4, 'Var5, 'Var6, 'Var7, 'Var8, 'NVVar1, 'NVVar2, 'NVVar3, 'NVVar4, 'Cat1, 'Cat2, 'Cat3, 'Cat4, 'Cat5, 'Cat6, 'Cat7, 'Cat8, 'Cat9, 'Cat10, 'Cat11, ... 4 more fields]\n+- Relation[7z��'\u001c\u0000\u0003���K�Ֆ\u0006\u0000\u0000\u0000\u0000a\u0000\u0000\u0000\u0000\u0000\u0000\u0000@�e�\u0000)\u001b��\u0000ora�A\\�\u0000e\u0004\u0004�G�Hg���\u0002�\u0005�G�'ۍі�S��c=w7��\u0011~�n�O@�a�o2f�t��U�VVP��\u0017ty��#3367,��W���2�'~X�l�6�y\u0018\u001e8q����e\u0001-��%.μ\u001ex�\u0006��\" �\u001cvhE��8?������K�#3368] csv\n\r\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:107)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:118)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$1.apply(QueryPlan.scala:122)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:122)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:127)\r\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:89)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\r\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:84)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:92)\r\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3301)\r\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1312)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9a0a5f8e6c04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m raw_df =raw_df.select('Vehicle','Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n\u001b[1;32m---> 10\u001b[1;33m                   'Cat1','Cat2','Cat3','Cat4','Cat5','Cat6','Cat7','Cat8','Cat9','Cat10','Cat11','Cat12','Calendar_Year','Model_Year','Claim_Amount')\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \"\"\"\n\u001b[1;32m-> 1202\u001b[1;33m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'cannot resolve \\'`Vehicle`\\' given input columns: [7z��\\'\\x1c\\x00\\x03���K�Ֆ\\x06\\x00\\x00\\x00\\x00a\\x00\\x00\\x00\\x00\\x00\\x00\\x00@�e�\\x00)\\x1b��\\x00ora�A\\\\�\\x00e\\x04\\x04�G�Hg���\\x02�\\x05�G�\\'ۍі�S��c=w7��\\x11~�n�O@�a�o2f�t��U�VVP��\\x17ty��, ��W���2�\\'~X�l�6�y\\x18\\x1e8q����e\\x01-��%.μ\\x1ex�\\x06��\" �\\x1cvhE��8?������K�];;\\n\\'Project [\\'Vehicle, \\'Var1, \\'Var2, \\'Var3, \\'Var4, \\'Var5, \\'Var6, \\'Var7, \\'Var8, \\'NVVar1, \\'NVVar2, \\'NVVar3, \\'NVVar4, \\'Cat1, \\'Cat2, \\'Cat3, \\'Cat4, \\'Cat5, \\'Cat6, \\'Cat7, \\'Cat8, \\'Cat9, \\'Cat10, \\'Cat11, ... 4 more fields]\\n+- Relation[7z��\\'\\x1c\\x00\\x03���K�Ֆ\\x06\\x00\\x00\\x00\\x00a\\x00\\x00\\x00\\x00\\x00\\x00\\x00@�e�\\x00)\\x1b��\\x00ora�A\\\\�\\x00e\\x04\\x04�G�Hg���\\x02�\\x05�G�\\'ۍі�S��c=w7��\\x11~�n�O@�a�o2f�t��U�VVP��\\x17ty��#3367,��W���2�\\'~X�l�6�y\\x18\\x1e8q����e\\x01-��%.μ\\x1ex�\\x06��\" �\\x1cvhE��8?������K�#3368] csv\\n'"
     ]
    }
   ],
   "source": [
    "#start Data pre-processing\n",
    "\n",
    "print('')\n",
    "print('Start data preprocessing...')\n",
    "print('')\n",
    "\n",
    "raw_df = spark.read.csv('./Dataset/ClaimPredictionChallenge/train_set.7z',header= True)\n",
    "\n",
    "raw_df =raw_df.select('Vehicle','Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1','Cat2','Cat3','Cat4','Cat5','Cat6','Cat7','Cat8','Cat9','Cat10','Cat11','Cat12','Calendar_Year','Model_Year','Claim_Amount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start Data pre-processing\n",
    "\n",
    "print('')\n",
    "print('Start data preprocessing...')\n",
    "print('')\n",
    "\n",
    "raw_df = spark.read.csv('./Dataset/ClaimPredictionChallenge/train_set.csv',header= True)\n",
    "\n",
    "raw_df =raw_df.select('Vehicle','Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1','Cat2','Cat3','Cat4','Cat5','Cat6','Cat7','Cat8','Cat9','Cat10','Cat11','Cat12','Calendar_Year','Model_Year','Claim_Amount')\n",
    "\n",
    "for col in raw_df.columns:\n",
    "    raw_df = raw_df.filter((raw_df[col] != '?'))\n",
    "    \n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "double_data = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4']\n",
    "int_data = ['Vehicle','Calendar_Year','Model_Year','Claim_Amount']\n",
    "input_features = double_data+int_data\n",
    "for col in double_data:\n",
    "    raw_df = raw_df.withColumn(col, raw_df[col].cast(DoubleType()))\n",
    "for col in int_data:\n",
    "    raw_df = raw_df.withColumn(col, raw_df[col].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.withColumn(int_data[1], (raw_df[int_data[1]]-2005))\n",
    "raw_df = raw_df.withColumn(int_data[2], (raw_df[int_data[2]]-1981))\n",
    "\n",
    "#categorical Nbr Lvls in Train for each cat\n",
    "categorical_features = {'Cat1':11,'Cat2':4,'Cat3':7,'Cat4':4,'Cat5':4,'Cat6':7,'Cat7':5,'Cat8':4,'Cat9':2,'Cat10':4,'Cat11':7,'Cat12':7}\n",
    "\n",
    "for col,num in categorical_features.items():\n",
    "    name = col+'_id'\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=name)\n",
    "    raw_df = indexer.fit(raw_df).transform(raw_df)\n",
    "\n",
    "raw_df = raw_df.select('Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8',\\\n",
    "                        'NVVar1','NVVar2','NVVar3','NVVar4',\\\n",
    "                        'Cat1_id','Cat2_id','Cat3_id','Cat4_id','Cat5_id','Cat6_id','Cat7_id','Cat8_id','Cat9_id','Cat10_id','Cat11_id','Cat12_id',\\\n",
    "                        'Calendar_Year','Model_Year','Claim_Amount')\n",
    "\n",
    "category_id = ['Cat1_id','Cat2_id','Cat3_id','Cat4_id','Cat5_id','Cat6_id','Cat7_id','Cat8_id','Cat9_id','Cat10_id','Cat11_id','Cat12_id']\n",
    "\n",
    "cat_ohe = []\n",
    "for col in category_id:\n",
    "    cat_ = col.replace('_id','_ohe')\n",
    "    cat_ohe.append(cat_)\n",
    "    input_features.append(cat_)\n",
    "\n",
    "\n",
    "data = raw_df.select('Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1_id','Cat2_id','Cat3_id','Cat4_id','Cat5_id','Cat6_id','Cat7_id','Cat8_id','Cat9_id','Cat10_id','Cat11_id','Cat12_id','Calendar_Year','Model_Year','Claim_Amount')\n",
    "\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=category_id, outputCols=cat_ohe)\n",
    "encoder_data = encoder.fit(data)\n",
    "data  = encoder_data.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = data .withColumn('weight',when((data['Claim_Amount'] != 0), 0.98).otherwise(0.02))\n",
    "data  = data .withColumn('not_zero',when((data['Claim_Amount'] != 0), 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.select('Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1_ohe','Cat2_ohe','Cat3_ohe','Cat4_ohe','Cat5_ohe','Cat6_ohe','Cat7_ohe','Cat8_ohe','Cat9_ohe','Cat10_ohe','Cat11_ohe','Cat12_ohe','Calendar_Year','Model_Year','Claim_Amount','weight','not_zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = ['Var1','Var2','Var3','Var4','Var5','Var6','Var7','Var8','NVVar1','NVVar2','NVVar3','NVVar4',\n",
    "                  'Cat1_ohe','Cat2_ohe','Cat3_ohe','Cat4_ohe','Cat5_ohe','Cat6_ohe','Cat7_ohe','Cat8_ohe','Cat9_ohe','Cat10_ohe','Cat11_ohe','Cat12_ohe','Calendar_Year','Model_Year']\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "feat_assembler = VectorAssembler(inputCols = features_list, outputCol = 'features')\n",
    "data = feat_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training......\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "import time\n",
    "data_logi= data.select('features','not_zero','weight','Claim_Amount')\n",
    "(trainingData, testData) = data_logi.randomSplit([0.7, 0.3], 47)\n",
    "print('Data preprocessing finished.')\n",
    "trainingData.cache()\n",
    "testData.cache()\n",
    "\n",
    "#classification\n",
    "start = time.time()\n",
    "print('Start training......')\n",
    "logistic_Reg = LogisticRegression(labelCol ='not_zero',weightCol = 'weight',maxIter = 20)\n",
    "logisticReg_model2 = logistic_Reg.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, not_zero: int, weight: double, Claim_Amount: int, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticReg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.583815\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "logisticReg_prediction = logisticReg_model2.transform(testData)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"not_zero\",metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(logisticReg_prediction)\n",
    "end = time.time()\n",
    "print('Logistic Regression Execution time:',end-start)\n",
    "print(\"auc = %g\" % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_notzero = trainingData.filter('not_zero != 0')\n",
    "test_notzero = testData.filter('not_zero != 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 171.76245760917664\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "glm_poisson = GeneralizedLinearRegression(featuresCol='features', labelCol='Claim_Amount', maxIter=10, regParam=0.01,\\\n",
    "                                          family='Gamma', link='identity')\n",
    "start = time.time()\n",
    "glm_model = glm_poisson.fit(train_notzero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select zero sample\n",
    "pred_zero = logisticReg_prediction.filter('prediction == 0')\n",
    "pred_zero = pred_zero.withColumn('claim_prediction',pred_zero['not_zero']*0).select('Claim_Amount','claim_prediction')\n",
    "\n",
    "#extract non zero value\n",
    "pred_nonzero = logisticReg_prediction.filter('prediction != 0')\n",
    "pred_nonzero = pred_nonzero.select('features','Claim_Amount')\n",
    "\n",
    "#compare model with non zero value\n",
    "pred_amount = glm_model.transform(pred_nonzero)\n",
    "pred_amount = pred_amount.select('Claim_Amount','prediction')\n",
    "pred_amount = pred_amount.withColumnRenamed('prediction','claim_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pred_amount.union(pred_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.withColumn('Claim_Amount', result['Claim_Amount'].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 39.8111\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Claim_Amount\", predictionCol=\"claim_prediction\", metricName=\"rmse\")\n",
    "glm_rmse = evaluator.evaluate(result)\n",
    "print(\"RMSE = %g\" % glm_rmse)\n",
    "end = time.time()\n",
    "print('GLM Execution time:',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Claim_Amount\", predictionCol=\"claim_prediction\", metricName=\"mae\")\n",
    "glm_mae = evaluator.evaluate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 1.57099\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE = %g\" % glm_mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
