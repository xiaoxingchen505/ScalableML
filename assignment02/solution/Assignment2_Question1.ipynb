{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an example of the solution for assignment 2 qestion 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import spark and create spark  session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import  numpy as np\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"COM6012 Assignment2 Question 1\") \\\n",
    "        .config(\"spark.local.dir\",\"/fastdata/acq18mc\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/fastdata/acq18mc/pyspark/spark-warehouse/\")\\\n",
    "        .config(\"hive.metastore.warehouse.dir\", \"/fastdata/acq18mc/pyspark/spark-warehouse/\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load csv gz  data as DataFrame\n",
    "print(\"start\")\n",
    "startTime = datetime.datetime.now()\n",
    "#load data as dataframe\n",
    "data = spark.read.option(\"inferschema\",False).csv(\"/fastdata/acq18mc/HIGGS.csv.gz\")\n",
    "#cache the data from later use\n",
    "data.cache()\n",
    "#register data to a temp table, for later sql use. \n",
    "data.registerTempTable(\"data\")\n",
    "print(f\"whole data size is {data.count()} loading data takes {(datetime.datetime.now()-startTime).total_seconds()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert string to double type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnName = data.columns\n",
    "\n",
    "labelColumnName = columnName[0]\n",
    "\n",
    "featureColumnName = columnName[1:]\n",
    "# composing a string with sql instructions to cast every column to double. \n",
    "convertToDoubleSQLString = \"\"\n",
    "for i, name in enumerate(columnName):\n",
    "    if i <len(columnName)-1:\n",
    "        convertToDoubleSQLString+=\"cast(\"+name+\" as double) ,\"\n",
    "    elif i==len(columnName)-1:\n",
    "        convertToDoubleSQLString+=\"cast(\"+name+\" as double)\"\n",
    "    else:\n",
    "        pass\n",
    "startTime = datetime.datetime.now()\n",
    "#execute the sql instruction\n",
    "doubleData = spark.sql(f\"select {convertToDoubleSQLString} from data\")\n",
    "doubleData.registerTempTable(\"doubleData\")\n",
    "doubleData.cache()\n",
    "print(f\"transfered to double, and takes {(datetime.datetime.now()- startTime).total_seconds()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert data to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert to vector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols = featureColumnName, outputCol = \"Feature\" )\n",
    "processedData = assembler.transform(doubleData)\n",
    "\n",
    "processedData.registerTempTable(\"TempData\")\n",
    "#change the column name to the spark ml models' default col names\n",
    "processedData = spark.sql(\"select _c0 as label, Feature as features from TempData\")\n",
    "print(\"get vector for feature \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset\n",
    "\n",
    " 1. extract a 5% subset and split it\n",
    " 2. split the whole dataset\n",
    " 3. cache each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "SEED=1234\n",
    "#get 5% of the whole data\n",
    "subSetData,_ = processedData.randomSplit([0.05, 0.95], seed = SEED)\n",
    "subSetData.cache()\n",
    "print(f\"sub set size is {subSetData.count()}\")\n",
    "\n",
    "#split the sub set\n",
    "(subTrainingData, subTestData) = subSetData.randomSplit([0.7, 0.3], seed = SEED)\n",
    "subTrainingData.cache()\n",
    "subTestData.cache()\n",
    "print(f\"sub set training set size is {subTrainingData.count()}\")\n",
    "#split the whole set\n",
    "trainingData, testData = processedData.randomSplit([0.6,0.4], seed = SEED)\n",
    "print(f\"whole training set size is {trainingData.count()}\")\n",
    "trainingData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training\n",
    " 1. check model name\n",
    " 2. setup param grid\n",
    " 3. cross validation on 5% dataset\n",
    " 4. extract the best pramaters and report classification results\n",
    " 5. train on the whole dataset and report the results\n",
    " 6. find the best three features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function is doing three repeated process, on three different models.\n",
    "# we use a modelName parameter and eval() to switch on  models.     \n",
    "def findBestParametersAndTrainAgain(modelName):\n",
    "    #check if input parameter is string\n",
    "    if not isinstance( modelName, str) :\n",
    "        raise Exception(\"Wrong Input\")\n",
    "    #if input string is one of the demanded model names\n",
    "    if  modelName not in [ \"RandomForestClassifier\", \"GBTClassifier\"] :\n",
    "        raise Exception(\"Wrong Input\")\n",
    "\n",
    "    \n",
    "    startTime = datetime.datetime.now()\n",
    "    \n",
    "    paramGrid = None\n",
    "    #create the model by given modelName\n",
    "    model = eval(modelName)(featuresCol='features', labelCol='label',predictionCol='prediction')\n",
    "    #we use binary evaluater for AUR, multi evaluator for accuracy\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    multiEvaluator = None\n",
    "  \n",
    "    #have a switch here to set up param grid for models. defferent model need different param grid\n",
    "             \n",
    "    if modelName == \"RandomForestClassifier\":\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(model.maxDepth, [10,5,15]) \\\n",
    "            .addGrid(model.maxBins, [32,20,15])\\\n",
    "            .addGrid(model.minInfoGain, [0.0,0.2,0.1])\\\n",
    "            .addGrid(model.impurity, ['gini','entropy'])\\\n",
    "            .build()\n",
    "        multiEvaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "        pipeline = Pipeline(stages=[ model])\n",
    "        \n",
    "    elif modelName == \"GBTClassifier\":\n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(model.maxDepth, [15, 10, 5]) \\\n",
    "            .addGrid(model.maxBins, [32, 20, 15])\\\n",
    "            .addGrid(model.minInfoGain, [0.0, 0.2, 0.3])\\\n",
    "            .build()\n",
    "        pipeline = Pipeline(stages=[model])\n",
    "        multiEvaluator = MulticlassClassificationEvaluator(metricName='accuracy',predictionCol='rawPrediction')\n",
    "    else:\n",
    "        raise Exception\n",
    "        \n",
    "    \n",
    "    crossVal = CrossValidator(estimator = pipeline, estimatorParamMaps = paramGrid, evaluator = evaluator , numFolds = 3 )\n",
    "    cvModel = crossVal.fit(subTrainingData)\n",
    "    prediction = cvModel.transform(subTestData)\n",
    "    print(f\"for {modelName}, cross validation best model  {evaluator.getMetricName()} :  {evaluator.evaluate(prediction)}, training takes time {(datetime.datetime.now()-startTime).total_seconds()} \")\n",
    "    if multiEvaluator:\n",
    "        print(f\"for {modelName} , acuracy is {multiEvaluator.evaluate(prediction)}\")\n",
    "    #get the best model parameters obtained from cross validation process. \n",
    "    bestPipeline = cvModel.bestModel\n",
    "    bestModel = bestPipeline.stages[0]\n",
    "    bestParams = bestModel.extractParamMap()\n",
    "    for param, value in bestParams.items():\n",
    "        print(f\"{param.name} : {value}\")\n",
    "    #create a new model to train it on the whole dataset, with the best parameter obtained above.\n",
    "    startTime = datetime.datetime.now()\n",
    "    newModel = eval(modelName)()\n",
    "    newModel = newModel.fit(trainingData,bestParams)\n",
    "    #transform test set\n",
    "    newPrediction = newModel.transform(testData)\n",
    "\n",
    "    print(f\"new model for {modelName}  {evaluator.getMetricName()} :  {evaluator.evaluate(newPrediction)}, training takes time {(datetime.datetime.now()-startTime).total_seconds()} \")\n",
    "\n",
    "    if multiEvaluator:\n",
    "        print(f\"new model for {modelName} , acuracy is {multiEvaluator.evaluate(newPrediction)}\")\n",
    "\n",
    "    # get feature importances or coefficients\n",
    "\n",
    "\n",
    "    if modelName == \"LogisticRegression\":\n",
    "        coefficients = newModel.coefficients\n",
    "        print(f\"get LR new model coefficients {coefficients} with length {len(coefficients)}\")\n",
    "        maxIndex = np.argmax(coefficients)\n",
    "        print(f\"LR model best feature index in {maxIndex} with coefficient {coefficients[maxIndex]}\")\n",
    "        \n",
    "    else:\n",
    "        featureImportances = list(newModel.featureImportances.toArray())\n",
    "        print(f\"{modelName} feature importance is {featureImportances} with length {len(featureImportances)}\")\n",
    "        maxIndex = np.argmax(featureImportances)\n",
    "        print(f\"{modelName} best feature index in {maxIndex} with importance  {featureImportances[maxIndex]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "findBestParametersAndTrainAgain(\"RandomForestClassifier\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findBestParametersAndTrainAgain(\"GBTClassifier\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
